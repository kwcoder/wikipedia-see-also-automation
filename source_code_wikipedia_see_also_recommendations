{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1jvMyrEDL350YYi2GkDQfjtaJXymx9u4_","timestamp":1732317606720}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Welcome to the Google Colab notebook for Team 15 - ML Mavericks, part of the Data Science Group Project Module at the University of Birmingham for the 2023/2024 cohort. This notebook contains all the code implementations for our system, designed to automate the creation of \"See Also\" sections in Wikipedia articles.**"],"metadata":{"id":"Br6chk8_0Qod"}},{"cell_type":"markdown","source":["# 1. Collecting and Preparing the Data\n"],"metadata":{"id":"O1Otw-iX0Ula"}},{"cell_type":"markdown","source":["### 1.1. Stage One: Selecting Articles' Titles"],"metadata":{"id":"xRxCmh7k0gLf"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Loading the TSV file with custom column names for easier understanding. (The clickstream data we used is available on this link: https://dumps.wikimedia.org/other/clickstream/2024-01/)\n","df = pd.read_csv(\"clickstream-enwiki-2024-01.tsv\", sep='\\t', names=['From', 'To', 'Type', 'Total Clicks'])\n","\n","# Just a quick check to see the first few rows and make sure our columns are correctly named.\n","print(df.head())\n"],"metadata":{"id":"CJDi4yHw0dzh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Grouping by destination page, summing total clicks, and sorting.\n","result_df = df.groupby('To')['Total Clicks'].sum().reset_index().sort_values(by='Total Clicks', ascending=False)\n","\n","# Renaming columns.\n","result_df.columns = ['Title', 'Total clicks']\n","\n","# Saving the result as a CSV file with the top 60k titles.\n","result_df.head(60000).to_csv('top_60k_titles.csv', index=False)\n"],"metadata":{"id":"Kh863Xow1VSA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.2. Stage Two: Delving into Article Features"],"metadata":{"id":"a5tNc8Rx3fRe"}},{"cell_type":"code","source":["import requests\n","import pandas as pd\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import time\n","\n","def fetch_xtools_article_info(project, article, session):\n","    article_formatted = article.replace(\" \", \"_\")\n","    url = f\"https://xtools.wmcloud.org/api/page/articleinfo/{project}/{article_formatted}\"\n","    response = session.get(url)\n","    if response.status_code == 200:\n","        data = response.json()\n","        return {\n","            \"revisions\": data.get(\"revisions\", 0),\n","            \"quality\": data.get(\"assessment\", {}).get(\"value\", \"Unknown\")\n","        }\n","    else:\n","        print(f\"Couldn't get data for {article}: HTTP {response.status_code}\")\n","        return {}\n","\n","def fetch_article_details(title, session):\n","    base_url = \"https://en.wikipedia.org/w/api.php\"\n","    params = {\n","        \"action\": \"query\",\n","        \"format\": \"json\",\n","        \"prop\": \"extracts|info|categories\",\n","        \"titles\": title,\n","        \"exintro\": \"\",\n","        \"explaintext\": \"\",\n","        \"inprop\": \"url\",\n","        \"cllimit\": \"max\",  # Getting max categories\n","        \"clshow\": \"!hidden\"  # Excluding hidden categories\n","    }\n","    info_response = session.get(base_url, params=params)\n","    if info_response.status_code == 200:\n","        info_data = info_response.json()\n","        page_id = next(iter(info_data['query']['pages']))\n","        page_info = info_data['query']['pages'][page_id]\n","\n","        # Extracting categories, removing 'Category:' prefix\n","        categories = [category['title'].replace('Category:', '').strip() for category in page_info.get('categories', []) if 'categories' in page_info]\n","\n","        total_views = 0\n","        views_url = f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/user/{title.replace(' ', '_')}/daily/20240101/20240131\"\n","        views_response = session.get(views_url)\n","        if views_response.status_code == 200:\n","            views_data = views_response.json()\n","            if 'items' in views_data:\n","                total_views = sum(item['views'] for item in views_data['items'])\n","            else:\n","                print(f\"No view data for {title}\")\n","        else:\n","            print(f\"Couldn't fetch view data for {title}: HTTP {views_response.status_code}\")\n","\n","        xtools_info = fetch_xtools_article_info(\"en.wikipedia.org\", title, session)\n","\n","        return {\n","            \"title\": title,\n","            \"size\": page_info.get('length', 0),\n","            \"total_views\": total_views,\n","            \"first_paragraph\": page_info.get('extract', \"\"),\n","            \"article_quality\": xtools_info.get(\"quality\", \"Unknown\"),\n","            \"article_categories\": categories  # Categories list without 'Category:' prefix, excluding hidden categories\n","        }\n","    else:\n","        print(f\"Couldn't fetch article details for {title}: HTTP {info_response.status_code}\")\n","        return {}\n","\n","def create_articles_df(titles, user_agent):\n","    articles_data = []\n","    with requests.Session() as session:\n","        session.headers.update({'User-Agent': user_agent})\n","        with ThreadPoolExecutor(max_workers=10) as executor:\n","            future_to_title = {executor.submit(fetch_article_details, title, session): title for title in titles}\n","            for future in as_completed(future_to_title):\n","                title = future_to_title[future]\n","                try:\n","                    article_details = future.result()\n","                    articles_data.append(article_details)\n","                except Exception as exc:\n","                    print(f\"{title} encountered an issue: {exc}\")\n","    return pd.DataFrame(articles_data)\n","\n","def process_batches(titles, user_agent, batch_size=100):\n","    for i in range(0, len(titles), batch_size):\n","        batch_titles = titles[i:i + batch_size]\n","        start_time = time.time()\n","        df = create_articles_df(batch_titles, user_agent)\n","        csv_filename = f\"batch_{i//batch_size + 1}_articles.csv\"\n","        df.to_csv(csv_filename, index=False)\n","        end_time = time.time()\n","        print(f\"Batch {i//batch_size + 1} done: {len(batch_titles)} articles saved to {csv_filename} in {end_time - start_time:.2f} seconds.\")\n","\n","# User agent information\n","user_agent = \"mah338@student.bham.ac.uk / University of Birmingham - Data Science Project\"\n","\n","# Load the file as CSV\n","top_60k_titles_csv = pd.read_csv(\"top_60k_titles.csv\")['Title'].tolist()\n","\n","# Process batches\n","process_batches(top_60k_titles_csv, user_agent)\n"],"metadata":{"id":"qO3zP6j23jNP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.3. The Final Dataset: Combining Files"],"metadata":{"id":"0_N0Q5Ug5LRL"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","# Assuming all CSV files are in the current directory\n","csv_files = [f\"batch_{i}_articles.csv\" for i in range(1, 603)]  # Generating list of CSV file names\n","\n","# Initializing an empty DataFrame\n","combined_df = pd.DataFrame()\n","\n","# Looping through each CSV file, reading it into a DataFrame, and appending it to combined_df\n","for file in csv_files:\n","    file_path = os.path.join(path_to_csv_files, file)\n","    if os.path.exists(file_path):  # Checking if the file exists\n","        temp_df = pd.read_csv(file_path)\n","        combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n","    else:\n","        print(f\"File {file} doesn't exist.\")\n","\n","# Now combined_df contains all the data from the 250 CSV files\n","print(f\"Combined DataFrame has {len(combined_df)} instances.\") #This is just to make sure\n","\n","# Saving the combined DataFrame to a new CSV file\n","combined_df.to_csv(\"600_field_combined.csv\", index=False)\n","\n","print(\"All files have been combined and saved to 250_field_combined.csv.\")\n"],"metadata":{"id":"-XASJfb55VNA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Exploratory Data Analysis"],"metadata":{"id":"bkcKorFYFifs"}},{"cell_type":"markdown","source":["## 2.1 An In-depth Examination of Article Size"],"metadata":{"id":"cDsvUWVYHOHc"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# Load the dataset\n","data = pd.read_csv('600_field_combined.csv')\n","\n","# Plot the histogram\n","plt.hist(data['size'], bins=50, color='grey', edgecolor='black')\n","plt.axvline(x=6000, color='black', linestyle='--', label='Threshold at 6K bytes')\n","plt.title('Article Size Distribution')\n","plt.xlabel('Size (bytes)')\n","plt.ylabel('Frequency')\n","plt.xlim(0, 350000)  # Limit x-axis to max value of 'size'\n","plt.legend()\n","\n","# Remove the box around the graph\n","plt.gca().spines['top'].set_visible(False)\n","plt.gca().spines['right'].set_visible(False)\n","plt.gca().spines['left'].set_visible(False)\n","plt.gca().spines['bottom'].set_visible(False)\n","\n","# Save the plot\n","plt.savefig('size.png', dpi=1000, bbox_inches='tight')\n","\n","# Show the plot\n","plt.show()\n","\n","# Find the bins with the highest frequencies\n","sorted_indices = counts.argsort()[::-1]  # Sort indices by count\n","max_count_bins = [(bins[i], bins[i + 1]) for i in sorted_indices[:2]]  # Get top two bins\n","max_count_values = [counts[i] for i in sorted_indices[:2]]  # Get counts of top two bins\n","\n","# Print info about top frequency bins\n","for i in range(len(max_count_bins)):\n","    print(f\"Bin {i+1}: Range {max_count_bins[i]} with a frequency of {max_count_values[i]}\")\n","\n"],"metadata":{"id":"R-8I9b3_HU9J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.2 An In-Depth Examination of Article Quality Distribution"],"metadata":{"id":"h-9sY0o3aCBV"}},{"cell_type":"code","source":["# Group article qualities, combining less common categories into \"Others\"\n","data['article_quality_grouped'] = data['article_quality'].apply(lambda x: x if x in ['FA', 'A', 'GA', 'B', 'C', 'Start', 'Stub'] else 'Others')\n","quality_counts_grouped = data['article_quality_grouped'].value_counts()[['FA', 'A', 'GA', 'B', 'C', 'Start', 'Stub', 'Others']]\n","\n","# Create bar plot\n","bars = plt.bar(quality_counts_grouped.index, quality_counts_grouped.values, color='grey', edgecolor='black')\n","plt.title('Article Quality Distribution', pad=20)\n","plt.xlabel('Article Quality')\n","plt.ylabel('Number of Articles')\n","plt.xticks(rotation=90)  # Rotate x-axis labels vertically\n","\n","# Add numbers on top of bars for better visibility\n","for bar in bars:\n","    yval = bar.get_height()\n","    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.05*yval, f'{int(yval)}',\n","             ha='center', va='bottom', rotation=0, color='black', fontsize=8, zorder=3)  # Set zorder to bring text to front\n","\n","# Remove the box around the graph\n","plt.gca().spines['top'].set_visible(False)\n","plt.gca().spines['right'].set_visible(False)\n","plt.gca().spines['left'].set_visible(False)\n","plt.gca().spines['bottom'].set_visible(False)\n","\n","# Adjust axis limits to make space for text\n","plt.ylim(0, max(quality_counts_grouped.values) * 1.03)\n","plt.tight_layout()\n","plt.savefig('quality.png', dpi=1000, bbox_inches='tight')\n","\n","plt.show()\n"],"metadata":{"id":"-ydEMkztaJb0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.3 An In-Depth Examination of Article Category Distribution"],"metadata":{"id":"svVzUUkUb3Sg"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from ast import literal_eval\n","from collections import Counter\n","\n","# Define a function to safely evaluate string literals\n","def safe_literal_eval(x):\n","    if isinstance(x, str):\n","        try:\n","            return literal_eval(x)\n","        except Exception:\n","            return []\n","    return x\n","\n","# Convert string representations of lists into actual lists\n","data['article_categories'] = data['article_categories'].apply(safe_literal_eval)\n","\n","# Adjusted category mapping using certain words\n","category_mapping = {\n","    'Sports': [\n","        'olympic sport', 'football', 'basketball', 'sportspeople', 'athletic', 'soccer', 'tennis',\n","        'athletics', 'baseball', 'rugby', 'cricket', 'volleyball', 'golf', 'swimming',\n","        'track and field', 'hockey', 'table tennis', 'badminton', 'skiing', 'snowboarding',\n","        'skating', 'cycling', 'boxing', 'mixed martial arts', 'wrestling', 'fencing', 'rowing',\n","        'sailing', 'equestrian', 'gymnastics', 'weightlifting', 'biathlon', 'triathlon', 'marathon',\n","        'sprint', 'judo', 'taekwondo', 'karate', 'archery', 'shooting sports', 'darts', 'bowling',\n","        'billiards', 'snooker', 'bodybuilding', 'surfing', 'motorsport', 'racing', 'figure skating',\n","        'doping in sports', 'sports nutrition', 'sports medicine', 'sports psychology',\n","        'physical training', 'sports equipment', 'team sports', 'individual sports', 'extreme sports',\n","        'water sports', 'winter sports', 'outdoor sports', 'indoor sports', 'professional sports',\n","        'amateur sports', 'college sports', 'youth sports', 'master sports', 'olympic games', 'world cup',\n","        'championships', 'sports leagues', 'sports teams', 'sportsmanship', 'coaching', 'sports strategy',\n","        'sports analytics', 'sports history', 'sports culture', 'fan culture', 'sports broadcasting',\n","        'sports journalism', 'sports awards', 'sports records', 'sports events', 'stadiums', 'arenas',\n","        'sports fans', 'athlete training', 'sports science'\n","    ],\n","    'Technology': [\n","        'software', 'hardware', 'internet', 'video game', 'computer', 'programming', 'ai',\n","        'artificial intelligence', 'gadget', 'mobile device', 'smartphone', 'tablet', 'laptop',\n","        'desktop', 'operating system', 'application', 'app development', 'user interface',\n","        'user experience', 'data science', 'machine learning', 'robotics', 'automation', 'blockchain',\n","        'cryptocurrency', 'cloud computing', 'big data', 'data analysis', 'networking',\n","        'cybersecurity', 'information security', 'hacking', 'ethical hacking', 'virtual reality',\n","        'augmented reality', 'drones', 'wearable technology', 'IoT', 'Internet of Things',\n","        'semiconductors', 'silicon chips', 'quantum computing', 'database', 'data management',\n","        'UI/UX design', 'web development', 'digital marketing', 'SEO', 'search engine optimization',\n","        'social media', 'e-commerce', 'fintech', 'financial technology', 'tech startup', 'innovation',\n","        'tech policy', 'privacy', 'tech ethics', 'software engineering', 'network infrastructure',\n","        'wireless technology', '5G', 'telecommunications', 'nanotechnology', 'biotechnology',\n","        'tech trends', 'gaming consoles', 'e-sports', 'tech reviews', 'tech tutorials',\n","        'coding languages', 'software development kit', 'SDK', 'open source', 'API',\n","        'application programming interface', 'tech integration', 'tech education', 'STEM',\n","        'science technology engineering mathematics', 'tech entrepreneurship', 'tech investment',\n","        'tech venture', 'tech gadgets', 'home automation', 'smart home', 'tech support',\n","        'tech forums', 'tech community', 'tech events', 'tech conferences', 'tech exhibitions'\n","    ],\n","    'Literature': [\n","        'novel', 'poetry', 'writer', 'book', 'literary genre', 'prose', 'drama', 'playwright',\n","        'short story', 'biography', 'essay', 'anthology', 'classic literature', 'literary criticism',\n","        'literary theory', 'non-fiction', 'fiction', 'science fiction', 'fantasy', 'mystery',\n","        'horror', 'historical novel', 'romance', 'graphic novel', 'comic book', 'memoir',\n","        'autobiography', 'epic', 'sonnet', 'haiku', 'limerick', 'ballad', 'literary device',\n","        'narrative structure', 'plot', 'character development', 'theme', 'motif', 'symbolism',\n","        'dialogue', 'rhetoric', 'satire', 'parody', 'allegory', 'critique', 'manuscript', 'publishing',\n","        'e-book', 'audiobook', 'translation', 'book series', 'author', 'poet', 'novelist', 'editor',\n","        'literary journal', 'book review', 'book club', 'reading', 'literature festival', 'literary award',\n","        'bestseller', 'classic', 'literary canon', 'literary movement', 'literary period', 'poetic form',\n","        'prose style', 'literary agent', 'book fair', 'public domain', 'copyright in literature',\n","        'academic writing', 'scholarly publication', 'creative writing', 'young adult literature',\n","        'children’s literature', 'oral tradition', 'folklore', 'mythology', 'literary scholarship',\n","        'text analysis', 'literature education', 'literary studies'\n","    ],\n","    'History': [\n","        'historical event', 'ancient history', '20th century', 'historian', 'medieval history',\n","        'world war', 'historical figure', 'modern history', 'renaissance', 'industrial revolution',\n","        'civilization', 'empire', 'kingdom', 'monarchy', 'dynasty', 'archaeology', 'artifact',\n","        'chronology', 'historiography', 'cultural heritage', 'historical research', 'military history',\n","        'political history', 'social history', 'economic history', 'history of science',\n","        'history of technology', 'art history', 'oral history', 'primary source', 'archive', 'documentary',\n","        'biography', 'timeline', 'genealogy', 'prehistoric', 'classical antiquity', 'colonialism',\n","        'revolution', 'exploration', 'historic site', 'museum', 'historical society', 'history education',\n","        'public history', 'historical reenactment', 'historical fiction', 'national history',\n","        'local history', 'global history', 'transnational history', 'history conference',\n","        'history publication', 'history journal', 'historical methodology', 'chronicle', 'epic',\n","        'folklore', 'mythology', 'ancient texts', 'inscription', 'paleography', 'numismatics',\n","        'heraldry', 'philately', 'cartography', 'historical maps', 'historical narrative',\n","        'historical analysis', 'historical period', 'historical drama', 'historical documentary',\n","        'age of discovery', 'age of enlightenment', 'middle ages', 'renaissance history', 'baroque',\n","        'classicism', 'romanticism', 'victorian era', 'modernism', 'postmodernism', 'contemporary history',\n","        'digital history', 'historical simulation'\n","    ],\n","    'Politics': [\n","        'politician', 'political party', 'election', 'government', 'democracy', 'political science',\n","        'legislation', 'public policy', 'governance', 'diplomacy', 'international relations',\n","        'political campaign', 'voting', 'civic engagement', 'civil rights', 'human rights',\n","        'political ideology', 'conservatism', 'liberalism', 'socialism', 'communism', 'anarchism',\n","        'federalism', 'parliamentary system', 'presidential system', 'monarchy', 'dictatorship',\n","        'geopolitics', 'political theory', 'political economy', 'political history', 'political philosophy',\n","        'political ethics', 'civic education', 'political analysis', 'political strategy', 'lobbying',\n","        'advocacy', 'activism', 'public administration', 'bureaucracy', 'electoral system',\n","        'political debate', 'political discourse', 'political leadership', 'nation-state', 'sovereignty',\n","        'nationalism', 'patriotism', 'political culture', 'political reform', 'political crisis',\n","        'campaign finance', 'voter turnout', 'political sociology', 'political psychology',\n","        'political communication', 'political commentary', 'political journalism', 'political satire',\n","        'political parties', 'independent politics', 'grassroots politics', 'party politics',\n","        'political movement', 'political coalition', 'government institution', 'public office',\n","        'political office', 'election law', 'political rights', 'political representation',\n","        'political negotiation', 'political advocacy', 'municipal politics', 'regional politics',\n","        'national politics', 'international politics', 'geopolitical conflict', 'political stability',\n","        'political change', 'policy analysis', 'public affairs', 'political consulting',\n","        'electoral politics', 'political management'\n","    ],\n","    'Science': [\n","        'biology', 'physics', 'chemistry', 'space', 'astronomy', 'earth science', 'environment',\n","        'genetics', 'botany', 'zoology', 'ecology', 'molecular biology', 'biochemistry',\n","        'microbiology', 'neuroscience', 'evolution', 'immunology', 'cellular biology',\n","        'quantum mechanics', 'thermodynamics', 'particle physics', 'nuclear physics',\n","        'astrophysics', 'cosmology', 'planetary science', 'geochemistry', 'geophysics',\n","        'meteorology', 'climatology', 'oceanography', 'paleontology', 'crystallography',\n","        'inorganic chemistry', 'organic chemistry', 'analytical chemistry', 'physical chemistry',\n","        'material science', 'science research', 'scientific method', 'experimental science',\n","        'scientific theory', 'scientific discovery', 'natural science', 'applied science',\n","        'interdisciplinary science', 'scientific community', 'scientific journal',\n","        'peer-reviewed research', 'laboratory', 'scientific experiment', 'science education',\n","        'science communication', 'science policy', 'science funding', 'science and technology',\n","        'science history', 'science ethics', 'environmental science', 'conservation biology',\n","        'wildlife science', 'earth systems', 'atmospheric science', 'space exploration',\n","        'rocket science', 'satellite technology', 'science innovation', 'science awards',\n","        'scientific breakthrough', 'scientific collaboration', 'science conference',\n","        'science exhibition', 'science debate', 'science advocacy', 'citizen science',\n","        'science literacy', 'science outreach', 'scientific literacy', 'science curriculum',\n","        'STEM education', 'scientific investigation', 'science fair', 'science festival',\n","        'science workshop', 'scientific inquiry', 'science news', 'science media'\n","    ],\n","    'Health': [\n","        'medicine', 'medical science', 'healthcare', 'nutrition', 'disease', 'psychology',\n","        'wellness', 'public health', 'epidemiology', 'pathology', 'pharmacology', 'anatomy',\n","        'physiology', 'genetics', 'oncology', 'cardiology', 'neurology', 'dermatology',\n","        'endocrinology', 'gastroenterology', 'immunology', 'ophthalmology', 'pediatrics',\n","        'psychiatry', 'radiology', 'surgery', 'veterinary medicine', 'nursing', 'dentistry',\n","        'mental health', 'preventive medicine', 'alternative medicine', 'holistic health',\n","        'sports medicine', 'physical therapy', 'occupational therapy', 'personal health', 'fitness',\n","        'exercise', 'weight management', 'diet', 'supplements', 'vitamins', 'mental wellness',\n","        'stress management', 'self-care', 'sleep hygiene', 'hygiene', 'sexual health',\n","        'reproductive health', 'women\"s health', 'men\"s health', 'pediatric health', 'geriatric health',\n","        'chronic conditions', 'infectious diseases', 'vaccination', 'public health policy',\n","        'health education', 'community health', 'health promotion', 'health insurance', 'medical research',\n","        'clinical trials', 'medical diagnostics', 'health technology', 'health informatics', 'e-health',\n","        'telemedicine', 'patient care', 'patient safety', 'medical ethics', 'health literacy',\n","        'health communication', 'health facilities', 'emergency medicine', 'critical care',\n","        'intensive care', 'health systems', 'global health', 'environmental health', 'health disparities',\n","        'health economics', 'health laws', 'health regulations', 'medical devices', 'medical imaging',\n","        'health data', 'medical records', 'health interventions', 'health outcomes', 'health risk factors',\n","        'health services', 'healthcare quality', 'healthcare access', 'healthcare management'\n","    ],\n","    'Entertainment': [\n","    'movies', 'television', 'music', 'theater', 'comedy', 'dance', 'pop culture', 'celebrities',\n","    'film industry', 'TV shows', 'documentaries', 'streaming services', 'live concerts', 'festivals',\n","    'awards shows', 'reality TV', 'animation', 'video games', 'board games', 'nightlife',\n","    'performing arts', 'opera', 'ballet', 'musical theatre', 'drama', 'sitcoms', 'talk shows',\n","    'radio', 'podcasts', 'audiobooks', 'music videos', 'songwriting', 'record labels',\n","    'music production', 'concert tours', 'theatre productions', 'cinematography', 'directing',\n","    'screenwriting', 'playwriting', 'acting', 'stand-up comedy', 'magic', 'circus',\n","    'celebrity gossip', 'fan clubs', 'fan conventions', 'cosplay', 'gaming', 'e-sports',\n","    'art exhibitions', 'museums', 'gallery shows', 'book readings', 'literary festivals',\n","    'entertainment news', 'media criticism', 'film criticism', 'music criticism', 'theatre criticism',\n","    'celebrity interviews', 'red carpet events', 'film festivals', 'entertainment technology',\n","    'VR in entertainment', 'AR in entertainment', 'special effects', 'visual effects',\n","    'production design', 'costume design', 'makeup artistry', 'choreography', 'talent shows',\n","    'variety shows', 'game shows', 'sports entertainment', 'interactive entertainment',\n","    'theme parks', 'amusement parks', 'carnivals', 'casinos', 'gambling', 'lotteries',\n","    'entertainment law', 'media studies', 'entertainment industry', 'celebrity culture',\n","    'entertainment history', 'cultural impact of entertainment', 'influencer culture', 'social media stars'\n","    ]\n","}\n","\n","def assign_general_category(specific_categories, mapping):\n","    general_categories = set()\n","    for specific in specific_categories:\n","        specific_words = specific.lower().split()  # Split into words and convert to lowercase for matching\n","        for general, specifics in mapping.items():\n","            for specific_word in specific_words:\n","                if any(specific_word == specific_mapped for specific_mapped in specifics):\n","                    general_categories.add(general)\n","    return list(general_categories)\n","\n","# Assign a general category to each article\n","data['general_category'] = data['article_categories'].apply(lambda x: assign_general_category(x, category_mapping))\n","\n","# Flatten the list of general categories for all articles to count them\n","all_general_categories = [category for sublist in data['general_category'] for category in sublist]\n","\n","# Count the occurrences of each general category\n","general_category_counts = Counter(all_general_categories)\n","\n","# Convert to a sorted list of tuples for easier plotting or analysis\n","sorted_general_category_counts = sorted(general_category_counts.items(), key=lambda x: x[1], reverse=True)\n","\n","# Unpack the category names and counts for plotting\n","categories, counts = zip(*sorted_general_category_counts)\n","\n","plt.figure(figsize=(10, 8))\n","plt.bar(categories, counts, color='grey', edgecolor='black')\n","plt.title('Article Counts by General Category')\n","plt.xlabel('General Category')\n","plt.ylabel('Number of Articles')\n","plt.xticks(rotation=45, ha=\"right\")\n","\n","# Remove the box around the graph\n","plt.gca().spines['top'].set_visible(False)\n","plt.gca().spines['right'].set_visible(False)\n","plt.gca().spines['left'].set_visible(False)\n","plt.gca().spines['bottom'].set_visible(False)\n","plt.savefig('Category.png', dpi=1000, bbox_inches='tight')\n","\n","plt.show()\n"],"metadata":{"id":"g8wrV3A7b4xB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.4. An In-Depth Examination of Article Views"],"metadata":{"id":"a9o0u9OTd8u9"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np  # This will be used for calculating histogram data\n","\n","# Calculate histogram data\n","counts, bin_edges = np.histogram(data['total_views'], bins=1000, range=(0, 300000))\n","\n","# Find the largest bin\n","largest_bin_index = np.argmax(counts)\n","largest_bin_count = counts[largest_bin_index]\n","largest_bin_range = (bin_edges[largest_bin_index], bin_edges[largest_bin_index + 1])\n","\n","# Plotting the histogram\n","plt.hist(data['total_views'], bins=1000, color='grey', edgecolor='black')\n","plt.title('Distribution of Total Views in January 2024', loc='right', pad=20)\n","plt.xlabel('Total Views')\n","plt.ylabel('Frequency')\n","plt.xlim(0, 300000)\n","\n","# Remove the box around the graph\n","plt.gca().spines['top'].set_visible(False)\n","plt.gca().spines['right'].set_visible(False)\n","plt.gca().spines['left'].set_visible(False)\n","plt.gca().spines['bottom'].set_visible(False)\n","\n","# Annotate the largest bin\n","vertical_offset = largest_bin_count * 19  # Applying a large offset\n","annotation_y_position = largest_bin_count + vertical_offset\n","\n","plt.text(largest_bin_range[0] + (largest_bin_range[1]-largest_bin_range[0])/2, annotation_y_position,\n","         f'{int(largest_bin_count)} articles\\n({int(largest_bin_range[0])}-{int(largest_bin_range[1])} views)',\n","         ha='center', va='bottom')\n","\n","plt.savefig('views.png', dpi=1000, bbox_inches='tight')\n","plt.show()\n"],"metadata":{"id":"OXcELrDyeBNs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. NLP and Semantic Vectors Generation"],"metadata":{"id":"eOayeg5jmwE-"}},{"cell_type":"markdown","source":["## 3.1. Semantic Vectors Generation Using BERT"],"metadata":{"id":"f2lruTzHw3TG"}},{"cell_type":"code","source":["# Note: We used Google Colab due to its computational efficiency.\n","\n","!pip install sentence-transformers\n","import pandas as pd\n","from sentence_transformers import SentenceTransformer\n","import time\n","import ast  # For converting string representations of lists into actual lists\n","\n","# Path to the CSV file\n","file_path = \"/content/drive/MyDrive/Data Science - UoB/Term 2/DS Project/Dataset and code file/21th Feb NLP/600_field_combined.csv\"\n","\n","df = pd.read_csv(file_path)\n","\n","# Selecting the 'title', 'first_paragraph', and 'article_categories' columns\n","df = df[['title', 'first_paragraph', 'article_categories']].iloc[0:60000].copy()\n","\n","# Load the BERT model for generating embeddings\n","model = SentenceTransformer('bert-base-nli-mean-tokens')\n","\n","# Convert 'article_categories' from string representation of list to a single string\n","def convert_categories_to_string(category_list_str):\n","=    if isinstance(category_list_str, str):\n","        categories = ast.literal_eval(category_list_str)\n","    else:\n","        categories = category_list_str\n","    return ', '.join(categories)\n","\n","# Apply the function to convert categories\n","df['article_categories'] = df['article_categories'].apply(convert_categories_to_string)\n","\n","# Concatenate title, first paragraph, and article categories into a single text column for embedding\n","df['text'] = df['title'].astype(str) + ' ' + df['first_paragraph'].astype(str) + ' ' + df['article_categories'].astype(str)\n","\n","# Generate BERT embeddings for each text instance\n","start_time = time.time()\n","embeddings = model.encode(df['text'].tolist(), show_progress_bar=True, convert_to_tensor=True)\n","end_time = time.time()\n","\n","# Create a DataFrame to store the semantic vectors\n","semantic_df = pd.DataFrame(embeddings.numpy(), columns=[f'bert_{i}' for i in range(len(embeddings[0]))])\n","\n","# Concatenate the semantic vectors DataFrame with the DataFrame containing the original features\n","new_df = pd.concat([\n","    df.reset_index(drop=True),\n","    semantic_df\n","], axis=1)\n","\n","# Calculate processing time\n","processing_time = end_time - start_time\n","print(f\"Processing time: {processing_time:.2f} seconds\")\n"],"metadata":{"id":"RJcD3ROHm3-7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.2. BERT Embeddings Validation through PCA"],"metadata":{"id":"_ktZ_KGMyKaF"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","data = pd.read_csv('600_field_combined_with_vectors.csv')\n","\n","#List of specified indices for articles to include\n","specified_indices = [521, 520, 221, 2290, 94, 186, 4541, 1759, 21329, 28794, 20170, 38807, 81, 165, 11, 49, 3152, 2254, 10369, 10817, 12882]\n","\n","# Select articles\n","sampled_data = data.loc[specified_indices]\n","\n","# Extract BERT embeddings\n","embeddings = sampled_data.loc[:, 'bert_0':'bert_767']\n","\n","# Perform PCA\n","pca = PCA(n_components=2)\n","reduced_embeddings = pca.fit_transform(embeddings)\n","\n","# Visualization\n","plt.figure(figsize=(12, 10))\n","for i, txt in enumerate(sampled_data['title']):\n","    if i not in [10, 2]:\n","        # Adjust vertical position of labels based on index\n","        vertical_position = reduced_embeddings[i, 1] + 0.01 if i % 2 == 0 else reduced_embeddings[i, 1] - 0.01\n","        plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1], color='grey', edgecolors='black', alpha=0.6)\n","        plt.text(reduced_embeddings[i, 0] + 0.01, vertical_position, txt, fontsize=10, color='black', ha='left', va='bottom' if i % 2 == 0 else 'top')\n","\n","plt.title('PCA of Article Vector Embeddings')\n","plt.xlabel('PCA Component 1')\n","plt.ylabel('PCA Component 2')\n","plt.grid(True, linestyle='--', linewidth=0.5)\n","plt.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n","plt.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n","\n","# Remove the box around the graph\n","plt.gca().spines['top'].set_visible(False)\n","plt.gca().spines['right'].set_visible(False)\n","plt.gca().spines['left'].set_visible(False)\n","plt.gca().spines['bottom'].set_visible(False)\n","plt.savefig('embeddings.png', dpi=1000, bbox_inches='tight')\n","\n","plt.show()\n"],"metadata":{"id":"YJJ7ci5dyOd3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Workflow of the Automated \"See Also\" System"],"metadata":{"id":"tI2eY3Ow0pid"}},{"cell_type":"markdown","source":["## 4.1. Input Article, Cosine Similarity Calculation, and Size Exclusion"],"metadata":{"id":"ca0qDiNI0_i1"}},{"cell_type":"code","source":["from sklearn.preprocessing import normalize\n","import pandas as pd\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","import pandas as pd\n","\n","file_name = 'full_dataset_with_vectors.csv'\n","\n","# Read the CSV file\n","data = pd.read_csv(file_name)\n","\n","# Extract these embeddings into a NumPy array\n","embeddings = data.loc[:, 'bert_0':'bert_767'].values\n","\n","# Normalize the embeddings to have unit length\n","norm_semantic_vectors = normalize(embeddings)\n","\n","def recommend_articles_with_info(current_article_index, data, top_n=20):\n","    # Compute cosine similarity\n","    similarities = cosine_similarity([norm_semantic_vectors[current_article_index]], norm_semantic_vectors)[0]\n","\n","    # Get indices of articles sorted by similarity (descending)\n","    sorted_indices = np.argsort(similarities)[::-1]\n","\n","    # Exclude the current article to prevent repetition\n","    sorted_indices = sorted_indices[sorted_indices != current_article_index]\n","\n","    # Get top 20 articles based on similarity ratio, excluding the current article\n","    top_20_indices = sorted_indices[:20]\n","\n","    # Filter out articles with less than 6K bytes from the top 20\n","    filtered_indices = [i for i in top_20_indices if data.iloc[i]['size'] >= 6000]\n","\n","    # Select desired features for recommendations\n","    selected_features = [\"title\", \"size\", \"total_views\", \"Introduction\", \"article_quality\", \"article_categories\"]\n","\n","    # Create DataFrame for recommendations including desired features and similarity scores\n","    recommendations_df = data.iloc[filtered_indices][:top_n][selected_features].copy()\n","    recommendations_df['Similarity Score'] = similarities[filtered_indices][:top_n]\n","\n","    # Adding current article info at the beginning\n","    current_article_info = pd.DataFrame({\n","        'title': [data.iloc[current_article_index]['title']],\n","        'size': [data.iloc[current_article_index]['size']],\n","        'total_views': [data.iloc[current_article_index]['total_views']],\n","        'Introduction': [data.iloc[current_article_index]['Introduction']],\n","        'article_quality': [data.iloc[current_article_index]['article_quality']],\n","        'article_categories': [data.iloc[current_article_index]['article_categories']],\n","        'Similarity Score': [np.nan]  # Current article won't have a similarity score with itself\n","    })\n","\n","    return pd.concat([current_article_info, recommendations_df], ignore_index=True)\n","\n","# This is an example usage\n","current_article_index = 301\n","recommendations_df = recommend_articles_with_info(current_article_index, data, top_n)\n","\n"],"metadata":{"id":"MDY_JQqk1MW0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.2. Sorting by Number of Shared Categories"],"metadata":{"id":"KA6A9arg2ZEI"}},{"cell_type":"code","source":["import ast\n","original_categories = ast.literal_eval(data.iloc[current_article_index]['article_categories'])\n","\n","# Function to count shared categories\n","def count_shared_categories(target_categories, original_categories):\n","    # Convert target categories from string representation of list to actual list\n","    target_categories_list = ast.literal_eval(target_categories)\n","    # Use set intersection to find common elements\n","    shared_categories = set(target_categories_list).intersection(set(original_categories))\n","    return len(shared_categories)\n","\n","# Apply the function to each row in the DataFrame to calculate NSC\n","recommendations_df['NSC'] = recommendations_df['article_categories'].apply(lambda x: count_shared_categories(x, original_categories))\n","\n","\n","# Sort by 'NSC' in descending order, then by 'Similarity Score' in descending order\n","# This ensures that articles with more shared categories come first\n","# For articles with the same number of shared categories, they are then sorted by their similarity score\n","recommendations_df = recommendations_df.sort_values(by=['NSC', 'Similarity Score'], ascending=[False, False])\n","\n","# Take the top 10 results after sorting\n","top_10_recommendations = recommendations_df.head(11)\n"],"metadata":{"id":"TV6Std6J2jxM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.3. Arranging by Views and Quality, and Final Selection"],"metadata":{"id":"RH0mknd-2khg"}},{"cell_type":"code","source":["# Extract the original article (assuming it is at index 0 of the DataFrame)\n","original_article = top_10_recommendations.iloc[:1]\n","\n","# Exclude the original article from the sorting process\n","rest_of_articles = top_10_recommendations.iloc[1:]\n","\n","# Sort the rest of the DataFrame by 'total_views' in descending order\n","sorted_rest = rest_of_articles.sort_values(by='total_views', ascending=False)\n","\n","# Concatenate the original article back at the top\n","sorted_df = pd.concat([original_article, sorted_rest])\n","\n","# Reset the index of the sorted DataFrame\n","sorted_df.reset_index(drop=True, inplace=True)\n","\n","\n","##Now for Quality:\n","#mapping from quality categories to numerical scores\n","quality_mapping = {\n","    'FA': 1, 'A': 2, 'GA': 3, 'B': 4, 'C': 5, 'Start': 6, 'Stub': 7,\n","    'FL': 8, 'AL': 9, 'BL': 10, 'CL': 11, 'NA': 12, '???': 13\n","}\n","\n","#Map article qualities to scores, assigning 14 to unrecognized categories\n","sorted_df['quality_score'] = sorted_df['article_quality'].apply(lambda x: quality_mapping.get(x, 14))\n","\n","#Exclude the original article for secondary sorting based on quality\n","rest_of_articles_after_quality = sorted_df.iloc[1:].sort_values(by=['quality_score', 'total_views'], ascending=[True, False])\n","\n","#Re-add the original article to maintain its position at the top\n","final_sorted_df = pd.concat([sorted_df.iloc[:1], rest_of_articles_after_quality])\n","\n","#Reset the index after sorting\n","final_sorted_df.reset_index(drop=True, inplace=True)\n","\n","#Display the final sorted DataFrame, prioritized by quality and then views\n","final_sorted_df.head(6)\n"],"metadata":{"id":"P0XrGdY62pIk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.4. Steps Combined"],"metadata":{"id":"4Mk2jyI83oRk"}},{"cell_type":"code","source":["from sklearn.preprocessing import normalize\n","import pandas as pd\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","import ast\n","\n","# Load the dataset\n","file_path = 'full_dataset_with_vectors.csv'\n","data = pd.read_csv(file_path)\n","\n","# Normalize the BERT embeddings\n","embeddings = data.loc[:, 'bert_0':'bert_767'].values\n","norm_embeddings = normalize(embeddings)\n","\n","def recommend_articles(article_index, data, top_n=5):\n","    # Compute cosine similarity\n","    similarities = cosine_similarity([norm_embeddings[article_index]], norm_embeddings)[0]\n","\n","    # Exclude the current article and sort others by similarity\n","    sorted_indices = np.argsort(similarities)[::-1][1:]\n","\n","    # Select the top 20 based on similarity\n","    top_20_indices = sorted_indices[:20]\n","\n","    # Filter out articles smaller than 6,000 bytes from the top 20\n","    filtered_indices = [i for i in top_20_indices if data.iloc[i]['size'] >= 6000]\n","\n","    # Calculate the number of shared categories for the filtered articles\n","    original_categories = set(ast.literal_eval(data.iloc[article_index]['article_categories']))\n","    shared_counts = []\n","    for i in filtered_indices:\n","        article_categories = set(ast.literal_eval(data.iloc[i]['article_categories']))\n","        shared_counts.append((i, len(original_categories.intersection(article_categories))))\n","\n","    # Sort by the number of shared categories, then by similarity, and take the top 10\n","    shared_counts.sort(key=lambda x: (-x[1], -similarities[x[0]]))\n","    top_10_indices = [i[0] for i in shared_counts][:10]\n","\n","    # Exclude the original article from the next steps\n","    if article_index in top_10_indices:\n","        top_10_indices.remove(article_index)\n","\n","    # Prepare the recommendations DataFrame\n","    recommendations = data.iloc[top_10_indices].copy()\n","    recommendations['similarity'] = similarities[top_10_indices]\n","\n","    # Sort by 'total_views' in descending order and select top 5\n","    recommendations = recommendations.sort_values(by='total_views', ascending=False).head(5)\n","\n","    # Map article qualities to numerical scores and sort\n","    quality_scores = {'FA': 1, 'A': 2, 'GA': 3, 'B': 4, 'C': 5, 'Start': 6, 'Stub': 7, 'List': 8}\n","    recommendations['quality_score'] = recommendations['article_quality'].map(lambda x: quality_scores.get(x, 9))\n","    final_recommendations = recommendations.sort_values(by='quality_score', ascending=True)\n","\n","    return final_recommendations\n","\n","# Example usage\n","article_index = 53\n","final_recommendations_df = recommend_articles(article_index, data, top_n=5)\n","final_recommendations_df\n"],"metadata":{"id":"gXPRC_3g3tLb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Current See Also Lists Extraction and Surveys Creation"],"metadata":{"id":"TPAj051K_97t"}},{"cell_type":"markdown","source":["## 5.1. Extracting Current See Also Sections For All Articles Using Web Scraping"],"metadata":{"id":"hEQMinTAEQ1m"}},{"cell_type":"code","source":["import requests\n","import pandas as pd\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from bs4 import BeautifulSoup  # For parsing HTML\n","import math\n","import time\n","\n","def fetch_section_content(title, session, section_title=\"See also\"):\n","    sections_url = \"https://en.wikipedia.org/w/api.php\"\n","    sections_params = {\n","        \"action\": \"parse\",\n","        \"page\": title,\n","        \"prop\": \"sections\",\n","        \"format\": \"json\"\n","    }\n","    sections_response = session.get(sections_url, params=sections_params)\n","    see_also_section_index = None\n","    if sections_response.status_code == 200:\n","        sections_data = sections_response.json()\n","        for section in sections_data[\"parse\"][\"sections\"]:\n","            if section[\"line\"].lower() == section_title.lower():\n","                see_also_section_index = section[\"index\"]\n","                break\n","\n","    see_also_articles = []\n","    if see_also_section_index:\n","        content_params = {\n","            \"action\": \"parse\",\n","            \"page\": title,\n","            \"section\": see_also_section_index,\n","            \"format\": \"json\",\n","            \"prop\": \"text\"\n","        }\n","        content_response = session.get(sections_url, params=content_params)\n","        if content_response.status_code == 200:\n","            content_data = content_response.json()\n","            html_content = content_data[\"parse\"][\"text\"][\"*\"]\n","            soup = BeautifulSoup(html_content, \"html.parser\")\n","            links = soup.find_all('a')\n","            for link in links:\n","                link_text = link.get_text().strip()\n","                if link_text and link_text.lower() != \"edit\":  # Filtering out 'edit' links and empty strings\n","                    see_also_articles.append(link_text)\n","\n","    return see_also_articles\n","\n","def fetch_article_see_also(title, session):\n","    see_also_articles = fetch_section_content(title, session)\n","    return {\n","        \"title\": title,\n","        \"see_also_articles\": see_also_articles,\n","        \"see_also_count\": len(see_also_articles)  # Count of see also articles\n","    }\n","\n","def create_articles_see_also_df(titles, user_agent):\n","    articles_data = []\n","    total_articles = len(titles)\n","    articles_processed = 0\n","    batch_start_time = time.time()\n","\n","    with requests.Session() as session:\n","        session.headers.update({'User-Agent': user_agent})\n","        with ThreadPoolExecutor(max_workers=10) as executor:\n","            future_to_title = {executor.submit(fetch_article_see_also, title, session): title for title in titles}\n","\n","            for future in as_completed(future_to_title):\n","                articles_processed += 1\n","                title = future_to_title[future]\n","                try:\n","                    article_see_also_details = future.result()\n","                    articles_data.append(article_see_also_details)\n","                except Exception as exc:\n","                    print(f\"{title} generated an exception: {exc}\")\n","\n","                # Update the progress and timing after every 250 articles processed\n","                if articles_processed % 250 == 0 or articles_processed == total_articles:\n","                    batch_end_time = time.time()\n","                    elapsed_time = batch_end_time - batch_start_time\n","                    progress_percentage = (articles_processed / total_articles) * 100\n","                    print(f\"Progress: {articles_processed}/{total_articles} articles processed ({math.floor(progress_percentage)}%). Time for batch: {elapsed_time:.2f} seconds.\")\n","                    batch_start_time = time.time()\n","\n","                # Save every 1000 articles as a CSV\n","                if articles_processed % 1000 == 0 or articles_processed == total_articles:\n","                    batch_df = pd.DataFrame(articles_data)\n","                    batch_file_name = f'articles_see_also_{articles_processed//1000}.csv'\n","                    batch_df.to_csv(batch_file_name, index=False)\n","                    print(f\"Saved {batch_file_name}\")\n","\n","    # The final DataFrame is saved after completing the loop\n","    if articles_processed % 1000 != 0:\n","        final_df = pd.DataFrame(articles_data)\n","        final_batch_number = (articles_processed // 1000) + 1\n","        final_file_name = f'articles_see_also_{final_batch_number}.csv'\n","        final_df.to_csv(final_file_name, index=False)\n","        print(f\"Saved {final_file_name}\")\n","\n","#usage\n","user_agent = \"mah338@student.bham.ac.uk\"\n","titles_list = data['title'].tolist()\n","df = create_articles_see_also_df(titles_list, user_agent)\n","df\n","\n"],"metadata":{"id":"sENKYuJCEe3T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.2. Using Google API Script to Create the Surveys"],"metadata":{"id":"YozCuN2VBC8I"}},{"cell_type":"code","source":["#This sheet is for the titles: https://docs.google.com/spreadsheets/d/1zX77iIUy4pq6M_KFBiKxjqpEz-8a3xNqR6fNRgOiIW0/edit?usp=sharing\n","#This sheet is for the lists titles: https://docs.google.com/spreadsheets/d/1wWrlIq8yIpBrOxQ0lhgFwBz6Cyec29eaCjZpvvmfmks/edit?usp=sharing\n","#This sheet is for the lists themselves (as images): https://drive.google.com/drive/folders/1fWgF28kQI7TUtR4GIdgEFzXm438l4rRT?usp=sharing\n","\n","function createSurveys() {\n","  const surveyTitlesSheetId = '1zX77iIUy4pq6M_KFBiKxjqpEz-8a3xNqR6fNRgOiIW0';\n","  const imageTitlesSheetId = '1wWrlIq8yIpBrOxQ0lhgFwBz6Cyec29eaCjZpvvmfmks';\n","  const driveFolderId = '1fWgF28kQI7TUtR4GIdgEFzXm438l4rRT';\n","  const start = 1; // Start index\n","  const end = 200; // End index\n","\n","  // Fetch survey titles for the first batch\n","  const surveyTitles = SpreadsheetApp.openById(surveyTitlesSheetId)\n","                        .getActiveSheet().getRange('A' + start + ':A' + end).getValues()\n","                        .flat();\n","\n","  // Fetch image captions for the first batch\n","  const imageCaptions = SpreadsheetApp.openById(imageTitlesSheetId)\n","                          .getActiveSheet().getRange('A' + start + ':A' + end).getValues()\n","                          .flat();\n","\n","  for (let i = 0; i < surveyTitles.length; i++) {\n","    const form = FormApp.create(surveyTitles[i])\n","                  .setDescription('Thank you for participating in our survey. Your participation will help us evaluate the system we have developed to automate the creation of the \"See Also\" section in Wikipedia articles using feature analysis and semantic analysis.\\n\\nBelow, you will find two lists. One of these lists was created by our system. Through this survey, we aim to understand how many users would prefer to click on an article from our generated list.\\n\\nPlease do not hesitate to contact us if you need further clarification or have any questions. You can reach us at this email: mah338@student.bham.ac.uk');\n","\n","    // Adjusting for the file naming convention with \".png\" extension\n","    const imageName = (i + start).toString() + '.png'; // Append \".png\" to match file names, adjusting index for batch\n","    let images = DriveApp.getFolderById(driveFolderId).getFilesByName(imageName);\n","    if (images.hasNext()) {\n","      let imageFile = images.next();\n","      let blob = imageFile.getBlob();\n","      let imgItem = form.addImageItem();\n","      imgItem.setImage(blob).setTitle(imageCaptions[i]).setAlignment(FormApp.Alignment.CENTER);\n","    } else {\n","      console.log(\"Image not found for title: \" + imageName);\n","    }\n","\n","    // Add questions\n","    form.addMultipleChoiceItem()\n","      .setTitle(\"Which list of 'See Also' articles do you prefer that contains articles you are more likely to click on for related information?\")\n","      .setChoiceValues(['List A', 'List B'])\n","      .setRequired(true);\n","\n","    form.addCheckboxItem()\n","      .setTitle(\"Please select the reason(s) for your choice. What aspects influenced your preference\")\n","      .setChoiceValues(['Relevance of articles to the main topic', 'Variety/diversity of articles presented', 'Popularity of articles presented'])\n","      .setRequired(true);\n","\n","    // Print the form link to the log\n","    console.log(\"Survey Created: \" + form.getTitle() + \" - \" + form.getPublishedUrl());\n","  }\n","}\n"],"metadata":{"id":"9hX9ZauvACmh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. Surveys Data Analysis"],"metadata":{"id":"nWzh1vRUGCbE"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from collections import Counter\n","\n","data = pd.read_csv('/content/Compiled - Surveys Results - ML Mavericks - Sheet1 [Google API Script].csv')\n","\n","total_responses = data.shape[0]\n","\n","# Count unique survey IDs\n","unique_surveys = data['id'].nunique()\n","\n","#This is for the Total surverys done, Unique Surveys, List Preference\n","# Distribution of list preferences\n","list_preference_counts = data['listPreference'].value_counts()\n","print(\"#\"*15)\n","print(\"Total Responses: \", total_responses)\n","print(\"#\"*15)\n","print(\"Unique Surveys: \", unique_surveys)\n","print(\"#\"*15)\n","print(\"List Preference: \", list_preference_counts)\n","\n","plt.figure(figsize=(8, 5))\n","list_preference_counts.plot(kind='bar', color=['blue', 'green'])\n","plt.title('Distribution of List Preferences in Survey Responses')\n","plt.xlabel('List Preference')\n","plt.ylabel('Number of Responses')\n","plt.xticks(rotation=0)\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.show()\n","\n","data['date'] = pd.to_datetime(data['timestamp']).dt.date\n","\n","# Count the number of surveys conducted each day\n","daily_surveys = data['date'].value_counts().sort_index()\n","\n","# Plotting the total surveys done per day\n","plt.figure(figsize=(12, 6))\n","daily_surveys.plot(kind='bar', color='purple')\n","plt.title('Total Surveys Conducted Per Day')\n","plt.xlabel('Date')\n","plt.ylabel('Number of Surveys')\n","plt.xticks(rotation=45)\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.tight_layout()\n","plt.show()\n","\n","##Surveys with Highest and Lowest Participation\n","\n","survey_participation = data['id'].value_counts()\n","\n","# Identifying the surveys with the highest and lowest participation\n","highest_participation = survey_participation.idxmax(), survey_participation.max()\n","lowest_participation = survey_participation.idxmin(), survey_participation.min()\n","print(\"#\"*15)\n","print(\"Highest Participation: \", highest_participation)\n","print(\"#\"*15)\n","print(\"Lowest Participation: \", lowest_participation)\n","\n","plt.figure(figsize=(16, 8))\n","ax = survey_participation.sort_index().plot(kind='bar', color='gray', fontsize=8)\n","plt.title('Number of Responses Per Survey ID')\n","plt.xlabel('Survey ID')\n","plt.ylabel('Number of Responses')\n","plt.axhline(y=highest_participation[1], color='red', linestyle='--', label=f'Highest (ID {highest_participation[0]})')\n","plt.axhline(y=lowest_participation[1], color='blue', linestyle='--', label=f'Lowest (ID {lowest_participation[0]})')\n","plt.legend()\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.tight_layout()\n","\n","# Annotate the highest and lowest participation IDs on the plot\n","ax.annotate(f'ID {highest_participation[0]}', xy=(highest_participation[0]-1, highest_participation[1]), xytext=(highest_participation[0]-1, highest_participation[1]+2),\n","             arrowprops=dict(facecolor='black', shrink=0.05), fontsize=10)\n","ax.annotate(f'ID {lowest_participation[0]}', xy=(lowest_participation[0]-1, lowest_participation[1]), xytext=(lowest_participation[0]-1, lowest_participation[1]+2),\n","             arrowprops=dict(facecolor='black', shrink=0.05), fontsize=10)\n","\n","plt.show()\n","\n","\n","#Time analysis of responses\n","data['time'] = pd.to_datetime(data['timestamp']).dt.time\n","data['day_of_week'] = pd.to_datetime(data['timestamp']).dt.day_name()\n","\n","# Plotting the number of responses per day over time\n","plt.figure(figsize=(14, 7))\n","daily_surveys.plot(kind='line', color='teal', marker='o')\n","plt.title('Number of Survey Responses Per Day Over Time')\n","plt.xlabel('Date')\n","plt.ylabel('Number of Responses')\n","plt.grid(True, linestyle='--', alpha=0.7)\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n","\n","data['hour'] = pd.to_datetime(data['timestamp']).dt.hour\n","\n","# Plotting the number of responses by hour of the day\n","plt.figure(figsize=(12, 6))\n","data['hour'].plot(kind='hist', bins=24, range=(0, 23), rwidth=0.8, color='darkblue', alpha=0.7)\n","plt.title('Distribution of Survey Responses by Time of Day')\n","plt.xlabel('Hour of the Day')\n","plt.ylabel('Number of Responses')\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.xticks(range(0, 24))\n","plt.tight_layout()\n","plt.show()\n","\n","weekly_trends = data['day_of_week'].value_counts().reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n","\n","# Plotting the number of responses by day of the week\n","plt.figure(figsize=(10, 5))\n","weekly_trends.plot(kind='bar', color='magenta')\n","plt.title('Survey Responses by Day of the Week')\n","plt.xlabel('Day of the Week')\n","plt.ylabel('Number of Responses')\n","plt.xticks(rotation=45)\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.show()\n","\n","\n","#This is for the Analysis of preferences (popularity, diversity, etc.\n","plt.figure(figsize=(10, 6))\n","reason_counts.plot(kind='bar', color=['cyan', 'orange', 'green'])\n","plt.title('Popularity of Reasons for List Preferences')\n","plt.xlabel('Reason')\n","plt.ylabel('Number of Mentions')\n","plt.xticks(rotation=45)\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.show()"],"metadata":{"id":"UI6zTIKiGLkQ"},"execution_count":null,"outputs":[]}]}